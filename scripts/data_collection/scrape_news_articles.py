#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è —Å–∫—Ä–∞–ø–∏–Ω–≥–∞ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π –æ –Ω–∞–ª–æ–≥–∞—Ö
"""

import os
import sys
import requests
from pathlib import Path
from urllib.parse import urljoin, urlparse
import time
import re
from datetime import datetime
import json

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –ø–∞–ø–∫—É –≤ –ø—É—Ç—å
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from app.config.settings import Settings


class NewsScraper:
    """–ö–ª–∞—Å—Å –¥–ª—è —Å–∫—Ä–∞–ø–∏–Ω–≥–∞ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π"""
    
    def __init__(self):
        self.settings = Settings()
        self.download_dir = Path("data/news_articles")
        self.download_dir.mkdir(parents=True, exist_ok=True)
        
        # –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π
        self.news_sources = [
            {
                "name": "Cinco_Dias",
                "url": "https://cincodias.elpais.com/tag/impuestos/",
                "base_url": "https://cincodias.elpais.com",
                "description": "Cinco D√≠as - Tax News"
            },
            {
                "name": "Expansion_Fiscalidad",
                "url": "https://www.expansion.com/economia/politica/fiscalidad.html",
                "base_url": "https://www.expansion.com",
                "description": "Expansi√≥n - Fiscalidad"
            },
            {
                "name": "El_Economista_Fiscal",
                "url": "https://www.eleconomista.es/economia/fiscal",
                "base_url": "https://www.eleconomista.es",
                "description": "El Economista - Fiscal"
            }
        ]
        
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
    
    def extract_articles_from_cinco_dias(self, html: str, base_url: str) -> list:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç–∞—Ç—å–∏ —Å Cinco D√≠as"""
        articles = []
        
        # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç–∞—Ç—å–∏
        article_links = re.findall(r'<a[^>]*href="([^"]*)"[^>]*class="[^"]*articulo[^"]*"[^>]*>', html, re.IGNORECASE)
        
        for link in article_links[:10]:  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 10
            if link.startswith('/'):
                full_url = urljoin(base_url, link)
            else:
                full_url = link
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
            title_match = re.search(r'<a[^>]*href="' + re.escape(link) + r'"[^>]*>([^<]+)</a>', html, re.IGNORECASE)
            title = title_match.group(1).strip() if title_match else "Sin t√≠tulo"
            
            articles.append({
                "title": title,
                "url": full_url,
                "source": "Cinco D√≠as"
            })
        
        return articles
    
    def extract_articles_from_expansion(self, html: str, base_url: str) -> list:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç–∞—Ç—å–∏ —Å Expansi√≥n"""
        articles = []
        
        # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç–∞—Ç—å–∏
        article_links = re.findall(r'<a[^>]*href="([^"]*)"[^>]*class="[^"]*articulo[^"]*"[^>]*>', html, re.IGNORECASE)
        
        for link in article_links[:10]:  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 10
            if link.startswith('/'):
                full_url = urljoin(base_url, link)
            else:
                full_url = link
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
            title_match = re.search(r'<a[^>]*href="' + re.escape(link) + r'"[^>]*>([^<]+)</a>', html, re.IGNORECASE)
            title = title_match.group(1).strip() if title_match else "Sin t√≠tulo"
            
            articles.append({
                "title": title,
                "url": full_url,
                "source": "Expansi√≥n"
            })
        
        return articles
    
    def extract_articles_from_economista(self, html: str, base_url: str) -> list:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç–∞—Ç—å–∏ —Å El Economista"""
        articles = []
        
        # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç–∞—Ç—å–∏
        article_links = re.findall(r'<a[^>]*href="([^"]*)"[^>]*class="[^"]*articulo[^"]*"[^>]*>', html, re.IGNORECASE)
        
        for link in article_links[:10]:  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 10
            if link.startswith('/'):
                full_url = urljoin(base_url, link)
            else:
                full_url = link
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
            title_match = re.search(r'<a[^>]*href="' + re.escape(link) + r'"[^>]*>([^<]+)</a>', html, re.IGNORECASE)
            title = title_match.group(1).strip() if title_match else "Sin t√≠tulo"
            
            articles.append({
                "title": title,
                "url": full_url,
                "source": "El Economista"
            })
        
        return articles
    
    def scrape_news_source(self, source_info: dict) -> list:
        """–°–∫—Ä–∞–ø–∏—Ç –æ–¥–∏–Ω –∏—Å—Ç–æ—á–Ω–∏–∫ –Ω–æ–≤–æ—Å—Ç–µ–π"""
        try:
            print(f"\nüì∞ –°–∫—Ä–∞–ø–∏–º: {source_info['name']}")
            print(f"   URL: {source_info['url']}")
            
            response = self.session.get(source_info['url'], timeout=30)
            response.raise_for_status()
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Ç–∞—Ç—å–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–∞
            if "cincodias" in source_info['url']:
                articles = self.extract_articles_from_cinco_dias(response.text, source_info['base_url'])
            elif "expansion" in source_info['url']:
                articles = self.extract_articles_from_expansion(response.text, source_info['base_url'])
            elif "eleconomista" in source_info['url']:
                articles = self.extract_articles_from_economista(response.text, source_info['base_url'])
            else:
                articles = []
            
            print(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ —Å—Ç–∞—Ç–µ–π: {len(articles)}")
            return articles
            
        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ —Å–∫—Ä–∞–ø–∏–Ω–≥–∞ {source_info['name']}: {e}")
            return []
    
    def scrape_all_sources(self) -> list:
        """–°–∫—Ä–∞–ø–∏—Ç –≤—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π"""
        print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º —Å–∫—Ä–∞–ø–∏–Ω–≥ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π")
        print("=" * 60)
        
        all_articles = []
        
        for source in self.news_sources:
            articles = self.scrape_news_source(source)
            all_articles.extend(articles)
            time.sleep(2)  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
        
        print(f"\nüìä –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ —Å—Ç–∞—Ç–µ–π: {len(all_articles)}")
        return all_articles
    
    def save_articles(self, articles: list) -> None:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å—Ç–∞—Ç—å–∏ –≤ JSON —Ñ–∞–π–ª"""
        if not articles:
            print("‚ùå –ù–µ—Ç —Å—Ç–∞—Ç–µ–π –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
            return
        
        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        for article in articles:
            article["scraped_at"] = datetime.now().isoformat()
            article["relevance_score"] = 0.8  # –ë–∞–∑–æ–≤—ã–π —Ä–µ–π—Ç–∏–Ω–≥
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ JSON
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"news_articles_{timestamp}.json"
        file_path = self.download_dir / filename
        
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(articles, f, ensure_ascii=False, indent=2)
        
        print(f"‚úÖ –°—Ç–∞—Ç—å–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {file_path}")
        print(f"üìä –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–∞—Ç–µ–π: {len(articles)}")
    
    def list_saved_articles(self) -> None:
        """–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π"""
        print("\nüìÅ –°–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏:")
        print("-" * 40)
        
        if not self.download_dir.exists():
            print("   –ü–∞–ø–∫–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
            return
        
        json_files = list(self.download_dir.glob("*.json"))
        if not json_files:
            print("   JSON —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return
        
        for file_path in sorted(json_files):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    articles = json.load(f)
                
                print(f"   üìÑ {file_path.name}")
                print(f"      –°—Ç–∞—Ç–µ–π: {len(articles)}")
                print(f"      –î–∞—Ç–∞: {articles[0].get('scraped_at', 'N/A')}")
                print(f"      –ü—É—Ç—å: {file_path}")
                
            except Exception as e:
                print(f"   ‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {file_path.name}: {e}")


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    scraper = NewsScraper()
    
    print("üîç TuExpertoFiscal NAIL - –°–∫—Ä–∞–ø–∏–Ω–≥ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π")
    print("=" * 60)
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏
    print("\nüìã –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π:")
    for i, source in enumerate(scraper.news_sources, 1):
        print(f"   {i}. {source['name']} - {source['description']}")
    
    # –°–∫—Ä–∞–ø–∏–º –≤—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
    articles = scraper.scrape_all_sources()
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    scraper.save_articles(articles)
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    scraper.list_saved_articles()


if __name__ == "__main__":
    main()
