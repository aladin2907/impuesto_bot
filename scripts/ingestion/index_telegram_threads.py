#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ Telegram —Ç—Ä–µ–¥–æ–≤ –≤ Elasticsearch
–°–æ–∑–¥–∞—ë—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤
"""

import json
import re
import argparse
from datetime import datetime
from typing import List, Dict, Any
from pathlib import Path

import os
import sys
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –ø–∞–ø–∫—É –ø—Ä–æ–µ–∫—Ç–∞ –≤ –ø—É—Ç—å
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from app.config.settings import Settings
from app.services.elasticsearch.elasticsearch_service import ElasticsearchService
from app.services.llm.llm_service import LLMService


class TelegramThreadIndexer:
    def __init__(self):
        self.settings = Settings()
        self.es_service = ElasticsearchService()
        self.llm_service = LLMService()
        # Ensure ES is reachable via HTTP service
        try:
            if not self.es_service.ping():
                raise RuntimeError("Failed to connect to Elasticsearch client")
        except Exception as e:
            print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å Elasticsearch –∫–ª–∏–µ–Ω—Ç: {e}")
            raise
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏
        self.tax_keywords = [
            '–∞–≤—Ç–æ–Ω–æ–º–æ', 'irpf', 'iva', 'ss', '—Å–æ—Ü–∏–∞–ª—å–Ω–æ–µ', '–Ω–∞–ª–æ–≥', 'tax', 
            'factura', '—Ñ–∞–∫—Ç—É—Ä–∞', 'declaracion', '–¥–µ–∫–ª–∞—Ä–∞—Ü–∏—è', 'retencion', 
            'retenci√≥n', 'cuota', '–∫–≤–æ—Ç–∞', 'trimestre', 'autonomo'
        ]
        
        self.visa_keywords = [
            'visa', '–≤–∏–∑–∞', 'residencia', '—Ä–µ–∑–∏–¥–µ–Ω—Ü–∏—è', 'nomad', '–Ω–æ–º–∞–¥', 
            'extranjero', '–∏–Ω–æ—Å—Ç—Ä–∞–Ω–µ—Ü', 'pasaporte', '–ø–∞—Å–ø–æ—Ä—Ç', 'nie', 
            'dni', '–ø–æ–ª–∏—Ü–∏—è', 'policia', '–≤–Ω–∂', '–≤—Ä–µ–º–µ–Ω–Ω–∞—è'
        ]
        
        self.business_keywords = [
            'empresa', '–∫–æ–º–ø–∞–Ω–∏—è', 'sociedad', '–æ–±—â–µ—Å—Ç–≤–æ', 'contrato', 
            '–∫–æ–Ω—Ç—Ä–∞–∫—Ç', 'empleado', '—Å–æ—Ç—Ä—É–¥–Ω–∏–∫', 'freelance', '—Ñ—Ä–∏–ª–∞–Ω—Å', 
            'cliente', '–∫–ª–∏–µ–Ω—Ç', '–±–∞–Ω–∫', 'bank', '—Å—á–µ—Ç', 'cuenta'
        ]

    def extract_keywords(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        if not text:
            return []
        
        text_lower = text.lower()
        keywords = []
        
        all_keywords = self.tax_keywords + self.visa_keywords + self.business_keywords
        
        for keyword in all_keywords:
            if keyword in text_lower:
                keywords.append(keyword)
        
        return list(set(keywords))

    def categorize_topics(self, keywords: List[str]) -> List[str]:
        """–ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∏—Ä—É–µ—Ç —Ç—Ä–µ–¥ –ø–æ —Ç–µ–º–∞–º"""
        topics = []
        
        if any(k in self.tax_keywords for k in keywords):
            topics.append('tax')
        if any(k in self.visa_keywords for k in keywords):
            topics.append('visa')
        if any(k in self.business_keywords for k in keywords):
            topics.append('business')
        
        return topics

    def calculate_quality_score(self, thread: Dict[str, Any]) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ —Ç—Ä–µ–¥–∞ –¥–ª—è –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏–∏"""
        score = 0.0
        
        # –ë–∞–∑–æ–≤—ã–π –±–∞–ª–ª –∑–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–æ–±—â–µ–Ω–∏–π
        message_count = thread.get('message_count', 1)
        score += min(message_count * 0.1, 2.0)  # –ú–∞–∫—Å–∏–º—É–º 2 –±–∞–ª–ª–∞
        
        # –ë–æ–Ω—É—Å –∑–∞ –¥–ª–∏–Ω–Ω—ã–µ —Ç—Ä–µ–¥—ã (–±–æ–ª—å—à–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞)
        if message_count > 5:
            score += 1.0
        elif message_count > 2:
            score += 0.5
        
        # –ë–æ–Ω—É—Å –∑–∞ –Ω–µ–¥–∞–≤–Ω–∏–µ —Ç—Ä–µ–¥—ã
        try:
            last_updated = datetime.fromisoformat(thread['last_updated'].replace('Z', '+00:00'))
            days_ago = (datetime.now().replace(tzinfo=last_updated.tzinfo) - last_updated).days
            
            if days_ago < 30:
                score += 1.0
            elif days_ago < 90:
                score += 0.5
        except:
            pass
        
        # –ë–æ–Ω—É—Å –∑–∞ –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        keywords = self.extract_keywords(' '.join([msg.get('text', '') for msg in thread.get('messages', [])]))
        if keywords:
            score += min(len(keywords) * 0.2, 1.0)  # –ú–∞–∫—Å–∏–º—É–º 1 –±–∞–ª–ª
        
        return min(score, 5.0)  # –ú–∞–∫—Å–∏–º—É–º 5 –±–∞–ª–ª–æ–≤

    def prepare_thread_for_indexing(self, thread: Dict[str, Any], group_name: str) -> Dict[str, Any]:
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç —Ç—Ä–µ–¥ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –≤ Elasticsearch"""
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Å–æ–æ–±—â–µ–Ω–∏—è
        all_texts = []
        for msg in thread.get('messages', []):
            if msg.get('text'):
                all_texts.append(msg['text'])
        
        content = ' '.join(all_texts)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        keywords = self.extract_keywords(content)
        
        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∏—Ä—É–µ–º —Ç–µ–º—ã
        topics = self.categorize_topics(keywords)
        
        # –í—ã—á–∏—Å–ª—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ
        quality_score = self.calculate_quality_score(thread)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
        try:
            first_date = datetime.fromisoformat(thread['first_message_date'].replace('Z', '+00:00'))
            year = first_date.year
            month = first_date.month
            quarter = f"Q{(month-1)//3 + 1}"
        except:
            year = 2024
            month = 1
            quarter = "Q1"
        
        return {
            'thread_id': thread['thread_id'],
            'group_name': group_name,
            'group_type': 'it_autonomos' if 'it' in group_name.lower() else 'nomads',
            'first_message_date': thread['first_message_date'],
            'last_updated': thread['last_updated'],
            'message_count': thread['message_count'],
            'max_depth': thread.get('max_depth', 0),
            
            # –û—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
            'content': content,
            'first_message': thread['messages'][0]['text'] if thread['messages'] else '',
            'last_message': thread['messages'][-1]['text'] if thread['messages'] else '',
            
            # –¢–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –ø–æ–ª—è
            'topics': topics,
            'keywords': keywords,
            'tax_related': any(k in self.tax_keywords for k in keywords),
            'visa_related': any(k in self.visa_keywords for k in keywords),
            'business_related': any(k in self.business_keywords for k in keywords),
            
            # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
            'year': year,
            'month': month,
            'quarter': quarter,
            
            # –ö–∞—á–µ—Å—Ç–≤–æ
            'quality_score': quality_score,
            'relevance_score': 0.0,  # –ë—É–¥–µ—Ç –æ–±–Ω–æ–≤–ª—è—Ç—å—Å—è –ø—Ä–∏ –ø–æ–∏—Å–∫–µ
            
            # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            'indexed_at': datetime.now().isoformat(),
            'source': 'telegram'
        }

    def create_index_mapping(self):
        """–°–æ–∑–¥–∞—ë—Ç mapping –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞ telegram_threads"""
        mapping = {
            "mappings": {
                "properties": {
                    "thread_id": {"type": "keyword"},
                    "group_name": {"type": "keyword"},
                    "group_type": {"type": "keyword"},
                    "first_message_date": {"type": "date"},
                    "last_updated": {"type": "date"},
                    "message_count": {"type": "integer"},
                    "max_depth": {"type": "integer"},
                    
                    # –û—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞
                    "content": {
                        "type": "text",
                        "analyzer": "standard",
                        "fields": {
                            "keyword": {"type": "keyword"},
                            "suggest": {"type": "completion"}
                        }
                    },
                    
                    # –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ–ª—è
                    "first_message": {"type": "text"},
                    "last_message": {"type": "text"},
                    
                    # –¢–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ç–µ–≥–∏
                    "topics": {
                        "type": "keyword",
                        "fields": {
                            "text": {"type": "text"}
                        }
                    },
                    
                    # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
                    "keywords": {"type": "keyword"},
                    "tax_related": {"type": "boolean"},
                    "visa_related": {"type": "boolean"},
                    "business_related": {"type": "boolean"},
                    
                    # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
                    "year": {"type": "integer"},
                    "month": {"type": "integer"},
                    "quarter": {"type": "keyword"},
                    
                    # –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –¥–ª—è RAG
                    "relevance_score": {"type": "float"},
                    "quality_score": {"type": "float"},
                    
                    # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                    "indexed_at": {"type": "date"},
                    "source": {"type": "keyword"}
                }
            }
        }
        
        return mapping

    def index_threads_from_file(self, file_path: str, group_name: str = None):
        """–ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç —Ç—Ä–µ–¥—ã –∏–∑ JSON —Ñ–∞–π–ª–∞"""
        print(f"üìÅ –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ {file_path}")
        
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        if isinstance(data, dict) and 'threads' in data:
            threads = data['threads']
            if not group_name:
                group_name = data.get('group_title', 'Unknown Group')
        else:
            threads = data
            if not group_name:
                group_name = Path(file_path).stem
        
        print(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(threads)} —Ç—Ä–µ–¥–æ–≤ –≤ –≥—Ä—É–ø–ø–µ '{group_name}'")
        
        # –°–æ–∑–¥–∞—ë–º –∏–Ω–¥–µ–∫—Å –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç (—á–µ—Ä–µ–∑ HTTP —Å–µ—Ä–≤–∏—Å)
        index_name = "telegram_threads"
        if not self.es_service.index_exists(index_name):
            print(f"üî® –°–æ–∑–¥–∞—ë–º –∏–Ω–¥–µ–∫—Å {index_name}")
            mapping = self.create_index_mapping()
            self.es_service.create_index(index_name, mapping)
        else:
            print(f"‚úÖ –ò–Ω–¥–µ–∫—Å {index_name} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
        
        # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º —Ç—Ä–µ–¥—ã –±–∞—Ç—á–∞–º–∏
        batch_size = 100
        indexed_count = 0
        
        for i in range(0, len(threads), batch_size):
            batch = threads[i:i + batch_size]
            prepared_batch = []
            
            for thread in batch:
                try:
                    prepared_thread = self.prepare_thread_for_indexing(thread, group_name)
                    prepared_batch.append(prepared_thread)
                except Exception as e:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ —Ç—Ä–µ–¥–∞ {thread.get('thread_id', 'unknown')}: {e}")
                    continue
            
            if prepared_batch:
                # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –±–∞—Ç—á
                try:
                    actions = []
                    for thread in prepared_batch:
                        action = {
                            "_index": index_name,
                            "_id": f"{group_name}_{thread['thread_id']}",
                            "_source": thread
                        }
                        actions.append(action)
                    
                    # Bulk —á–µ—Ä–µ–∑ HTTP —Å–µ—Ä–≤–∏—Å
                    docs = []
                    for a in actions:
                        doc = a["_source"].copy()
                        doc["_id"] = a["_id"]
                        docs.append(doc)
                    self.es_service.bulk_index(index_name, docs)
                    indexed_count += len(docs)
                    print(f"üì§ –ò–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ {len(docs)} —Ç—Ä–µ–¥–æ–≤ (–≤—Å–µ–≥–æ: {indexed_count}/{len(threads)})")
                        
                except Exception as e:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –±–∞—Ç—á–∞: {e}")
        
        print(f"‚úÖ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –í—Å–µ–≥–æ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ: {indexed_count} —Ç—Ä–µ–¥–æ–≤")
        return indexed_count

    def test_search(self, query: str, limit: int = 5):
        """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –ø–æ–∏—Å–∫ –ø–æ –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º —Ç—Ä–µ–¥–∞–º"""
        print(f"üîç –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ–∏—Å–∫: '{query}'")
        
        search_body = {
            "query": {
                "multi_match": {
                    "query": query,
                    "fields": ["content^2", "first_message^1.5", "last_message^1.5"],
                    "type": "best_fields",
                    "fuzziness": "AUTO"
                }
            },
            "size": limit,
            "_source": ["thread_id", "group_name", "content", "topics", "quality_score", "first_message_date"]
        }
        
        try:
            results = self.es_service.search(
                index_name="telegram_threads",
                query=search_body["query"],
                size=limit
            )
            print(f"üìä –ù–∞–π–¥–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(results)}")
            for hit in results:
                score = hit.get("_score", 0)
                print(f"\nüéØ Score: {score:.2f}")
                print(f"Thread: {hit.get('thread_id')} ({hit.get('group_name')})")
                print(f"Topics: {hit.get('topics', [])}")
                print(f"Quality: {hit.get('quality_score', 0):.1f}")
                print(f"Content: {hit.get('content','')[:200]}...")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ: {e}")


def main():
    parser = argparse.ArgumentParser(description='–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è Telegram —Ç—Ä–µ–¥–æ–≤ –≤ Elasticsearch')
    parser.add_argument('file', help='–ü—É—Ç—å –∫ JSON —Ñ–∞–π–ª—É —Å —Ç—Ä–µ–¥–∞–º–∏')
    parser.add_argument('--group-name', help='–ù–∞–∑–≤–∞–Ω–∏–µ –≥—Ä—É–ø–ø—ã (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ —Ñ–∞–π–ª–∞)')
    parser.add_argument('--test-search', help='–¢–µ—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫ –ø–æ—Å–ª–µ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏')
    parser.add_argument('--limit', type=int, default=5, help='–õ–∏–º–∏—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞')
    
    args = parser.parse_args()
    
    indexer = TelegramThreadIndexer()
    
    try:
        # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º —Ç—Ä–µ–¥—ã
        indexed_count = indexer.index_threads_from_file(args.file, args.group_name)
        
        # –¢–µ—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫ –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω
        if args.test_search:
            indexer.test_search(args.test_search, args.limit)
            
    except Exception as e:
        print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())
