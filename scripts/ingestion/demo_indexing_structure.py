#!/usr/bin/env python3
"""
–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ Telegram —Ç—Ä–µ–¥–æ–≤
–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∫–∞–∫ –±—É–¥—É—Ç –≤—ã–≥–ª—è–¥–µ—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ Elasticsearch
"""

import json
import re
from datetime import datetime
from typing import List, Dict, Any
from pathlib import Path


class TelegramDataAnalyzer:
    def __init__(self):
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏
        self.tax_keywords = [
            '–∞–≤—Ç–æ–Ω–æ–º–æ', 'irpf', 'iva', 'ss', '—Å–æ—Ü–∏–∞–ª—å–Ω–æ–µ', '–Ω–∞–ª–æ–≥', 'tax', 
            'factura', '—Ñ–∞–∫—Ç—É—Ä–∞', 'declaracion', '–¥–µ–∫–ª–∞—Ä–∞—Ü–∏—è', 'retencion', 
            'retenci√≥n', 'cuota', '–∫–≤–æ—Ç–∞', 'trimestre', 'autonomo'
        ]
        
        self.visa_keywords = [
            'visa', '–≤–∏–∑–∞', 'residencia', '—Ä–µ–∑–∏–¥–µ–Ω—Ü–∏—è', 'nomad', '–Ω–æ–º–∞–¥', 
            'extranjero', '–∏–Ω–æ—Å—Ç—Ä–∞–Ω–µ—Ü', 'pasaporte', '–ø–∞—Å–ø–æ—Ä—Ç', 'nie', 
            'dni', '–ø–æ–ª–∏—Ü–∏—è', 'policia', '–≤–Ω–∂', '–≤—Ä–µ–º–µ–Ω–Ω–∞—è'
        ]
        
        self.business_keywords = [
            'empresa', '–∫–æ–º–ø–∞–Ω–∏—è', 'sociedad', '–æ–±—â–µ—Å—Ç–≤–æ', 'contrato', 
            '–∫–æ–Ω—Ç—Ä–∞–∫—Ç', 'empleado', '—Å–æ—Ç—Ä—É–¥–Ω–∏–∫', 'freelance', '—Ñ—Ä–∏–ª–∞–Ω—Å', 
            'cliente', '–∫–ª–∏–µ–Ω—Ç', '–±–∞–Ω–∫', 'bank', '—Å—á–µ—Ç', 'cuenta'
        ]

    def extract_keywords(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        if not text:
            return []
        
        text_lower = text.lower()
        keywords = []
        
        all_keywords = self.tax_keywords + self.visa_keywords + self.business_keywords
        
        for keyword in all_keywords:
            if keyword in text_lower:
                keywords.append(keyword)
        
        return list(set(keywords))

    def categorize_topics(self, keywords: List[str]) -> List[str]:
        """–ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∏—Ä—É–µ—Ç —Ç—Ä–µ–¥ –ø–æ —Ç–µ–º–∞–º"""
        topics = []
        
        if any(k in self.tax_keywords for k in keywords):
            topics.append('tax')
        if any(k in self.visa_keywords for k in keywords):
            topics.append('visa')
        if any(k in self.business_keywords for k in keywords):
            topics.append('business')
        
        return topics

    def calculate_quality_score(self, thread: Dict[str, Any]) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ —Ç—Ä–µ–¥–∞ –¥–ª—è –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏–∏"""
        score = 0.0
        
        # –ë–∞–∑–æ–≤—ã–π –±–∞–ª–ª –∑–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–æ–±—â–µ–Ω–∏–π
        message_count = thread.get('message_count', 1)
        score += min(message_count * 0.1, 2.0)  # –ú–∞–∫—Å–∏–º—É–º 2 –±–∞–ª–ª–∞
        
        # –ë–æ–Ω—É—Å –∑–∞ –¥–ª–∏–Ω–Ω—ã–µ —Ç—Ä–µ–¥—ã (–±–æ–ª—å—à–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞)
        if message_count > 5:
            score += 1.0
        elif message_count > 2:
            score += 0.5
        
        # –ë–æ–Ω—É—Å –∑–∞ –Ω–µ–¥–∞–≤–Ω–∏–µ —Ç—Ä–µ–¥—ã
        try:
            last_updated = datetime.fromisoformat(thread['last_updated'].replace('Z', '+00:00'))
            days_ago = (datetime.now().replace(tzinfo=last_updated.tzinfo) - last_updated).days
            
            if days_ago < 30:
                score += 1.0
            elif days_ago < 90:
                score += 0.5
        except:
            pass
        
        # –ë–æ–Ω—É—Å –∑–∞ –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        keywords = self.extract_keywords(' '.join([msg.get('text', '') for msg in thread.get('messages', [])]))
        if keywords:
            score += min(len(keywords) * 0.2, 1.0)  # –ú–∞–∫—Å–∏–º—É–º 1 –±–∞–ª–ª
        
        return min(score, 5.0)  # –ú–∞–∫—Å–∏–º—É–º 5 –±–∞–ª–ª–æ–≤

    def prepare_thread_for_indexing(self, thread: Dict[str, Any], group_name: str) -> Dict[str, Any]:
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç —Ç—Ä–µ–¥ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –≤ Elasticsearch"""
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Å–æ–æ–±—â–µ–Ω–∏—è
        all_texts = []
        for msg in thread.get('messages', []):
            if msg.get('text'):
                all_texts.append(msg['text'])
        
        content = ' '.join(all_texts)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        keywords = self.extract_keywords(content)
        
        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∏—Ä—É–µ–º —Ç–µ–º—ã
        topics = self.categorize_topics(keywords)
        
        # –í—ã—á–∏—Å–ª—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ
        quality_score = self.calculate_quality_score(thread)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
        try:
            first_date = datetime.fromisoformat(thread['first_message_date'].replace('Z', '+00:00'))
            year = first_date.year
            month = first_date.month
            quarter = f"Q{(month-1)//3 + 1}"
        except:
            year = 2024
            month = 1
            quarter = "Q1"
        
        return {
            'thread_id': thread['thread_id'],
            'group_name': group_name,
            'group_type': 'it_autonomos' if 'it' in group_name.lower() else 'nomads',
            'first_message_date': thread['first_message_date'],
            'last_updated': thread['last_updated'],
            'message_count': thread['message_count'],
            'max_depth': thread.get('max_depth', 0),
            
            # –û—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
            'content': content,
            'first_message': thread['messages'][0]['text'] if thread['messages'] else '',
            'last_message': thread['messages'][-1]['text'] if thread['messages'] else '',
            
            # –¢–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –ø–æ–ª—è
            'topics': topics,
            'keywords': keywords,
            'tax_related': any(k in self.tax_keywords for k in keywords),
            'visa_related': any(k in self.visa_keywords for k in keywords),
            'business_related': any(k in self.business_keywords for k in keywords),
            
            # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
            'year': year,
            'month': month,
            'quarter': quarter,
            
            # –ö–∞—á–µ—Å—Ç–≤–æ
            'quality_score': quality_score,
            'relevance_score': 0.0,  # –ë—É–¥–µ—Ç –æ–±–Ω–æ–≤–ª—è—Ç—å—Å—è –ø—Ä–∏ –ø–æ–∏—Å–∫–µ
            
            # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            'indexed_at': datetime.now().isoformat(),
            'source': 'telegram'
        }

    def analyze_file(self, file_path: str, sample_size: int = 10):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ñ–∞–π–ª –∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ø—Ä–∏–º–µ—Ä—ã —Å—Ç—Ä—É–∫—Ç—É—Ä—ã"""
        print(f"üìÅ –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ñ–∞–π–ª: {file_path}")
        
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        if isinstance(data, dict) and 'threads' in data:
            threads = data['threads']
            group_name = data.get('group_title', 'Unknown Group')
        else:
            threads = data
            group_name = Path(file_path).stem
        
        print(f"üìä –í—Å–µ–≥–æ —Ç—Ä–µ–¥–æ–≤: {len(threads)}")
        print(f"üè∑Ô∏è –ì—Ä—É–ø–ø–∞: {group_name}")
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–∏–º–µ—Ä—ã
        analyzer = TelegramDataAnalyzer()
        
        # –ë–µ—Ä—ë–º —Ç—Ä–µ–¥—ã —Å —Ä–∞–∑–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Å–æ–æ–±—â–µ–Ω–∏–π
        sample_threads = []
        
        # –û–¥–∏–Ω–æ—á–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
        single_threads = [t for t in threads if t['message_count'] == 1][:2]
        sample_threads.extend(single_threads)
        
        # –°—Ä–µ–¥–Ω–∏–µ —Ç—Ä–µ–¥—ã
        medium_threads = [t for t in threads if 2 <= t['message_count'] <= 5][:3]
        sample_threads.extend(medium_threads)
        
        # –î–ª–∏–Ω–Ω—ã–µ —Ç—Ä–µ–¥—ã
        long_threads = [t for t in threads if t['message_count'] > 10][:2]
        sample_threads.extend(long_threads)
        
        print(f"\nüîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º {len(sample_threads)} –ø—Ä–∏–º–µ—Ä–æ–≤ —Ç—Ä–µ–¥–æ–≤:")
        
        for i, thread in enumerate(sample_threads):
            print(f"\n{'='*60}")
            print(f"–¢–†–ï–î #{i+1} (ID: {thread['thread_id']})")
            print(f"{'='*60}")
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
            indexed_thread = analyzer.prepare_thread_for_indexing(thread, group_name)
            
            print(f"üìä –°–æ–æ–±—â–µ–Ω–∏–π: {indexed_thread['message_count']}")
            print(f"üìÖ –î–∞—Ç–∞: {indexed_thread['first_message_date']}")
            print(f"üè∑Ô∏è –¢–µ–º—ã: {indexed_thread['topics']}")
            print(f"üîë –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {indexed_thread['keywords']}")
            print(f"‚≠ê –ö–∞—á–µ—Å—Ç–≤–æ: {indexed_thread['quality_score']:.1f}/5.0")
            print(f"üí∞ –ù–∞–ª–æ–≥–æ–≤–∞—è —Ç–µ–º–∞: {indexed_thread['tax_related']}")
            print(f"üõÇ –í–∏–∑–æ–≤–∞—è —Ç–µ–º–∞: {indexed_thread['visa_related']}")
            print(f"üíº –ë–∏–∑–Ω–µ—Å —Ç–µ–º–∞: {indexed_thread['business_related']}")
            
            print(f"\nüìù –ü–µ—Ä–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ:")
            print(f"   {indexed_thread['first_message'][:150]}...")
            
            if indexed_thread['message_count'] > 1:
                print(f"\nüìù –ü–æ—Å–ª–µ–¥–Ω–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ:")
                print(f"   {indexed_thread['last_message'][:150]}...")
            
            print(f"\nüìÑ –ü–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç (–ø–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤):")
            print(f"   {indexed_thread['content'][:200]}...")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –≤—Å–µ–º —Ç—Ä–µ–¥–∞–º
        print(f"\n{'='*60}")
        print("–°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –í–°–ï–ú –¢–†–ï–î–ê–ú")
        print(f"{'='*60}")
        
        all_keywords = []
        topic_counts = {'tax': 0, 'visa': 0, 'business': 0}
        quality_scores = []
        
        for thread in threads[:1000]:  # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–µ—Ä–≤—ã–µ 1000
            indexed_thread = analyzer.prepare_thread_for_indexing(thread, group_name)
            
            all_keywords.extend(indexed_thread['keywords'])
            
            for topic in indexed_thread['topics']:
                if topic in topic_counts:
                    topic_counts[topic] += 1
            
            quality_scores.append(indexed_thread['quality_score'])
        
        # –¢–æ–ø –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        from collections import Counter
        keyword_counts = Counter(all_keywords)
        
        print(f"\nüîë –¢–æ–ø –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤:")
        for keyword, count in keyword_counts.most_common(10):
            print(f"   {keyword}: {count}")
        
        print(f"\nüè∑Ô∏è –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ç–µ–º–∞–º:")
        for topic, count in topic_counts.items():
            print(f"   {topic}: {count}")
        
        print(f"\n‚≠ê –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞:")
        print(f"   –°—Ä–µ–¥–Ω–µ–µ: {sum(quality_scores)/len(quality_scores):.2f}")
        print(f"   –ú–∞–∫—Å–∏–º—É–º: {max(quality_scores):.2f}")
        print(f"   –ú–∏–Ω–∏–º—É–º: {min(quality_scores):.2f}")


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='–ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã Telegram —Ç—Ä–µ–¥–æ–≤')
    parser.add_argument('file', help='–ü—É—Ç—å –∫ JSON —Ñ–∞–π–ª—É —Å —Ç—Ä–µ–¥–∞–º–∏')
    parser.add_argument('--sample-size', type=int, default=10, help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞')
    
    args = parser.parse_args()
    
    analyzer = TelegramDataAnalyzer()
    analyzer.analyze_file(args.file, args.sample_size)


if __name__ == "__main__":
    main()

