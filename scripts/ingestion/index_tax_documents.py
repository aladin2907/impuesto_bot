#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –Ω–∞–ª–æ–≥–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (HTML) –≤ Elasticsearch
–†–∞–∑–±–∏–≤–∞–µ—Ç HTML –Ω–∞ —á–∞–Ω–∫–∏ –∏ –∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
"""

import os
import sys
import json
import re
from datetime import datetime
from typing import List, Dict, Any
from pathlib import Path
from bs4 import BeautifulSoup

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –ø–∞–ø–∫—É –ø—Ä–æ–µ–∫—Ç–∞ –≤ –ø—É—Ç—å
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from app.config.settings import Settings
from app.services.elasticsearch.elasticsearch_service import ElasticsearchService


class TaxDocumentIndexer:
    def __init__(self):
        self.settings = Settings()
        self.es_service = ElasticsearchService()
        
        # –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ ES
        if not self.es_service.ping():
            raise RuntimeError("Failed to connect to Elasticsearch")
        
        # –ú–∞–ø–ø–∏–Ω–≥ –Ω–∞–∑–≤–∞–Ω–∏–π —Ñ–∞–π–ª–æ–≤ –∫ —Ç–∏–ø–∞–º –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        self.document_types = {
            "LGT_General_Tax_Law": "Ley General Tributaria",
            "IRPF_Personal_Income_Tax": "IRPF - Personal Income Tax",
            "IVA_Value_Added_Tax": "IVA - Value Added Tax",
            "Impuesto_Sociedades_Corporate_Tax": "Corporate Tax",
            "Impuesto_Sucesiones_Donaciones": "Inheritance & Gift Tax",
            "Impuesto_Patrimonio_Wealth_Tax": "Wealth Tax"
        }
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏
        self.tax_categories = {
            "autonomo": ["aut√≥nomo", "autonomo", "self-employed", "freelance"],
            "empresa": ["empresa", "sociedad", "company", "corporation"],
            "iva": ["iva", "vat", "value added tax", "impuesto sobre el valor a√±adido"],
            "irpf": ["irpf", "personal income tax", "renta", "income tax"],
            "retenciones": ["retenci√≥n", "retencion", "withholding", "retention"],
            "declaraciones": ["declaraci√≥n", "declaracion", "declaration", "return"],
            "sanciones": ["sanci√≥n", "sancion", "penalty", "fine", "multa"],
            "procedimientos": ["procedimiento", "procedure", "process", "tramite"]
        }

    def extract_text_from_html(self, html_content: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —á–∏—Å—Ç—ã–π —Ç–µ–∫—Å—Ç –∏–∑ HTML"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # –£–¥–∞–ª—è–µ–º —Å–∫—Ä–∏–ø—Ç—ã –∏ —Å—Ç–∏–ª–∏
            for script in soup(["script", "style"]):
                script.decompose()
            
            # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç
            text = soup.get_text()
            
            # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç
            lines = (line.strip() for line in text.splitlines())
            chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
            text = ' '.join(chunk for chunk in chunks if chunk)
            
            return text
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ HTML: {e}")
            return html_content

    def split_text_into_chunks(self, text: str, chunk_size: int = 1000, overlap: int = 200) -> List[str]:
        """–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º"""
        if len(text) <= chunk_size:
            return [text]
        
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + chunk_size
            
            # –ò—â–µ–º –≥—Ä–∞–Ω–∏—Ü—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–ª—è –ª—É—á—à–µ–≥–æ —Ä–∞–∑–±–∏–µ–Ω–∏—è
            if end < len(text):
                # –ò—â–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Ç–æ—á–∫—É, –≤–æ—Å–∫–ª–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π –∏–ª–∏ –≤–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –∑–Ω–∞–∫
                for i in range(end, max(start + chunk_size - 100, start), -1):
                    if text[i] in '.!?':
                        end = i + 1
                        break
            
            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
            
            start = end - overlap
            if start >= len(text):
                break
        
        return chunks

    def extract_categories(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        text_lower = text.lower()
        categories = []
        
        for category, keywords in self.tax_categories.items():
            if any(keyword in text_lower for keyword in keywords):
                categories.append(category)
        
        return categories

    def prepare_chunk_for_indexing(self, chunk: str, chunk_index: int, document_info: Dict[str, Any]) -> Dict[str, Any]:
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç —á–∞–Ω–∫ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏"""
        categories = self.extract_categories(chunk)
        
        return {
            "document_id": document_info["document_id"],
            "chunk_id": f"{document_info['document_id']}_chunk_{chunk_index}",
            "chunk_index": chunk_index,
            
            # –û—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
            "content": chunk,
            
            # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            "document_title": document_info["title"],
            "document_type": document_info["type"],
            "document_number": document_info.get("number", ""),
            
            # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏ —Ç–µ–≥–∏
            "categories": categories,
            "tax_related": len(categories) > 0,
            
            # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
            "indexed_at": datetime.now().isoformat(),
            "source": "tax_documents"
        }

    def create_index_mapping(self):
        """–°–æ–∑–¥–∞—ë—Ç mapping –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞ pdf_documents"""
        mapping = {
            "settings": {
                "number_of_shards": 1,
                "number_of_replicas": 1,
                "analysis": {
                    "analyzer": {
                        "spanish_text": {
                            "type": "custom",
                            "tokenizer": "standard",
                            "filter": ["lowercase", "spanish_stop", "spanish_stemmer"]
                        }
                    },
                    "filter": {
                        "spanish_stop": {
                            "type": "stop",
                            "stopwords": "_spanish_"
                        },
                        "spanish_stemmer": {
                            "type": "stemmer",
                            "language": "spanish"
                        }
                    }
                }
            },
            "mappings": {
                "properties": {
                    "document_id": {"type": "keyword"},
                    "chunk_id": {"type": "keyword"},
                    "chunk_index": {"type": "integer"},
                    
                    # –û—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞
                    "content": {
                        "type": "text",
                        "analyzer": "spanish_text",
                        "fields": {
                            "keyword": {"type": "keyword"},
                            "exact": {
                                "type": "text",
                                "analyzer": "standard"
                            }
                        }
                    },
                    
                    # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                    "document_title": {
                        "type": "text",
                        "analyzer": "spanish_text"
                    },
                    "document_type": {"type": "keyword"},
                    "document_number": {"type": "keyword"},
                    
                    # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏ —Ç–µ–≥–∏
                    "categories": {"type": "keyword"},
                    "tax_related": {"type": "boolean"},
                    
                    # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
                    "indexed_at": {"type": "date"},
                    "source": {"type": "keyword"}
                }
            }
        }
        
        return mapping

    def index_document_file(self, file_path: str) -> int:
        """–ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç –æ–¥–∏–Ω HTML —Ñ–∞–π–ª"""
        print(f"üìÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º: {file_path}")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ
        filename = Path(file_path).stem
        document_type = self.document_types.get(filename, "Unknown")
        
        # –ß–∏—Ç–∞–µ–º HTML —Ñ–∞–π–ª
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                html_content = f.read()
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ {file_path}: {e}")
            return 0
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
        text = self.extract_text_from_html(html_content)
        print(f"   üìä –†–∞–∑–º–µ—Ä —Ç–µ–∫—Å—Ç–∞: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏
        chunks = self.split_text_into_chunks(text)
        print(f"   üì¶ –°–æ–∑–¥–∞–Ω–æ —á–∞–Ω–∫–æ–≤: {len(chunks)}")
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
        document_info = {
            "document_id": filename,
            "title": document_type,
            "type": "tax_law",
            "number": self.extract_document_number(text)
        }
        
        prepared_chunks = []
        for i, chunk in enumerate(chunks):
            prepared_chunk = self.prepare_chunk_for_indexing(chunk, i, document_info)
            prepared_chunks.append(prepared_chunk)
        
        # –°–æ–∑–¥–∞—ë–º –∏–Ω–¥–µ–∫—Å –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        index_name = "pdf_documents"
        if not self.es_service.index_exists(index_name):
            print(f"üî® –°–æ–∑–¥–∞—ë–º –∏–Ω–¥–µ–∫—Å {index_name}")
            mapping = self.create_index_mapping()
            self.es_service.create_index(index_name, mapping)
        else:
            print(f"‚úÖ –ò–Ω–¥–µ–∫—Å {index_name} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
        
        # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º —á–∞–Ω–∫–∏ –±–∞—Ç—á–∞–º–∏
        batch_size = 50
        indexed_count = 0
        
        for i in range(0, len(prepared_chunks), batch_size):
            batch = prepared_chunks[i:i + batch_size]
            
            # –î–æ–±–∞–≤–ª—è–µ–º _id –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞
            for chunk in batch:
                chunk["_id"] = chunk["chunk_id"]
            
            try:
                self.es_service.bulk_index(index_name, batch)
                indexed_count += len(batch)
                print(f"   üì§ –ò–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ {len(batch)} —á–∞–Ω–∫–æ–≤ (–≤—Å–µ–≥–æ: {indexed_count}/{len(prepared_chunks)})")
            except Exception as e:
                print(f"   ‚ùå –û—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –±–∞—Ç—á–∞: {e}")
        
        print(f"   ‚úÖ –î–æ–∫—É–º–µ–Ω—Ç –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω: {indexed_count} —á–∞–Ω–∫–æ–≤")
        return indexed_count

    def extract_document_number(self, text: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–æ–º–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Ç–∏–ø–∞ "Ley 58/2003" –∏–ª–∏ "Real Decreto 123/2024"
        patterns = [
            r'Ley\s+(\d+/\d+)',
            r'Real\s+Decreto\s+(\d+/\d+)',
            r'Decreto\s+(\d+/\d+)',
            r'Orden\s+(\d+/\d+)',
            r'Resoluci√≥n\s+(\d+/\d+)'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return match.group(1)
        
        return ""

    def index_all_documents(self, documents_dir: str) -> int:
        """–ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç –≤—Å–µ HTML –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –ø–∞–ø–∫–µ"""
        print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é –Ω–∞–ª–æ–≥–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        print("=" * 60)
        
        docs_path = Path(documents_dir)
        if not docs_path.exists():
            print(f"‚ùå –ü–∞–ø–∫–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {documents_dir}")
            return 0
        
        # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ HTML —Ñ–∞–π–ª—ã
        html_files = list(docs_path.glob("*.html"))
        if not html_files:
            print(f"‚ùå HTML —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ {documents_dir}")
            return 0
        
        print(f"üìÅ –ù–∞–π–¥–µ–Ω–æ {len(html_files)} HTML —Ñ–∞–π–ª–æ–≤")
        
        total_chunks = 0
        for html_file in html_files:
            chunks_count = self.index_document_file(str(html_file))
            total_chunks += chunks_count
            print()
        
        print("=" * 60)
        print(f"‚úÖ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –í—Å–µ–≥–æ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ: {total_chunks} —á–∞–Ω–∫–æ–≤")
        return total_chunks

    def test_search(self, query: str, limit: int = 5):
        """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –ø–æ–∏—Å–∫ –ø–æ –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º"""
        print(f"üîç –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ–∏—Å–∫: '{query}'")
        
        search_query = {
            "multi_match": {
                "query": query,
                "fields": ["content^2", "document_title^1.5"],
                "type": "best_fields",
                "fuzziness": "AUTO"
            }
        }
        
        try:
            results = self.es_service.search(
                index_name="pdf_documents",
                query=search_query,
                size=limit
            )
            
            print(f"üìä –ù–∞–π–¥–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(results)}")
            
            for hit in results:
                score = hit.get("_score", 0)
                print(f"\nüéØ Score: {score:.2f}")
                print(f"Document: {hit.get('document_title')} ({hit.get('document_id')})")
                print(f"Categories: {hit.get('categories', [])}")
                print(f"Content: {hit.get('content', '')[:200]}...")
                
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ: {e}")


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –Ω–∞–ª–æ–≥–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ Elasticsearch')
    parser.add_argument('--documents-dir', default='data/tax_documents', help='–ü–∞–ø–∫–∞ —Å HTML –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏')
    parser.add_argument('--test-search', help='–¢–µ—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫ –ø–æ—Å–ª–µ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏')
    parser.add_argument('--limit', type=int, default=5, help='–õ–∏–º–∏—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞')
    
    args = parser.parse_args()
    
    indexer = TaxDocumentIndexer()
    
    try:
        # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
        total_chunks = indexer.index_all_documents(args.documents_dir)
        
        # –¢–µ—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫ –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω
        if args.test_search and total_chunks > 0:
            indexer.test_search(args.test_search, args.limit)
            
    except Exception as e:
        print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())
